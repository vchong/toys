#!/usr/bin/env python3

'''
96btool - Tool to study a users forum posts

Prerequisites (ubuntu:16.04):

 * sudo apt -y install python3 python3-pip
 * sudo apt -y install python3-iso8601 python3-lxml python3-requests-cache

Reminders of interesting threads for a weekly report:

    96btool fetch --since 'next-friday -13 days' --nick danielt | \
    96btool filter --until 'next-friday -1 week' | \
    96btool weekly

Create list of links in case the reminders are not enough:

    96btool fetch --since 'next-friday -13 days' --nick danielt | \
    96btool filter --until 'next-friday -1 week' | \
    96btool format --template '{uri}' | cut -f1 -d'#' | sort -u


If you want to develop the tool itself to pull additional information from
the HTML you may need to experiment interactively with "real" HTML objects.
Try:

import requests
from lxml import html
page = requests.get('http://www.96boards.org/members/danielt/forums/replies')
tree = html.fromstring(page.content)
content = tree.xpath('//div[@class="bbp-reply-content"]')

'''

import argparse
import collections
import datetime
import iso8601
import hashlib
import json
import re
import requests
import subprocess
import sys
import os
import textwrap
import time
from lxml import html
from lxml import etree

# If it's installed we'd rather use IPython for interaction...
try:
	import IPython
	interact = IPython.embed
except:
	import pdb
	interact = pdb.set_trace

# Let's make the plotting tools optional (for those who only use glimpse
# for their weekly report)
try:
	import matplotlib.pyplot as plt
except:
	pass

# Try to enable caching
try:
        import requests_cache
        requests_cache.install_cache()
except:
        # Provide a nop implementation
        class requests_cache:
                def __enter__(self):
                        pass
                def __exit__(self, exception_type, exception_value, traceback):
                        pass
                @staticmethod
                def disabled():
                        return requests_cache()

def get_colour(s):
	override = {
		'lava' : 'orange',
		'LAVA' : 'orange',
		'96boards' : 'red',
		'96Boards' : 'red',
	}

	if s in override:
		return override[s]

	return '#{}'.format(hashlib.md5(str(s).encode('UTF-8')).hexdigest()[-6:])

def collect_by(things, categorize):
	results = collections.defaultdict(list)
	for t in things:
		results[categorize(t)].append(t)
	return results

def collect_and_count_by(things, categorize, count=lambda t: 1):
	results = collections.defaultdict(int)
	for t in things:
		results[categorize(t)] += count(t)
	return results

def stacked_bar_chart(things, filename, title=None, xlabel=None, ylabel=None):
	x_labels = sorted(things.keys())

	legend_labels = set()
	for y in things.values():
		legend_labels |= set(y.keys())
	legend_labels = sorted(legend_labels)

	plots = []
	left = range(len(x_labels))

	# Working in reverse order allows us to stack things so that the
	# (alphabetic) legend is geometrically consistant with the stacks
	# on the charts.
	cumulative_height = [ 0 for x in x_labels ]
	for y in sorted(legend_labels, reverse=True):
		values = [ things[x][y] for x in x_labels ]
		plots.append(plt.bar(left, values, 1,
			bottom=cumulative_height, color=get_colour(y)))
		cumulative_height = [ c+v for c, v in zip(cumulative_height,
							  values) ]

	# Now we must reverse plots in order to match it back up with the
	# legend labels
	plots.reverse()

	if title:
		plt.title(title)
	if xlabel:
		plt.xlabel(xlabel)
	if ylabel:
		plt.ylabel(ylabel)

	plt.xticks([ x+0.5 for x in left ], x_labels, rotation=90)
	lgd = plt.legend(plots, legend_labels, bbox_to_anchor=(1.05, 1), loc=2)
	plt.savefig(filename, bbox_extra_artists=(lgd,), bbox_inches='tight')
	plt.close()

def parse_date(s, end_of_day=False):
	'''Convert a string to a datetime object.

	Operates by calling out to date (because it has a good relative date
	parser).

	'''
	date = subprocess.check_output(["date", "-d", s, "+%Y-%m-%d %H:%M:%S"])
	date = str(date, 'utf-8')
	date = date.rstrip()
	date = datetime.datetime.strptime(date, "%Y-%m-%d %H:%M:%S")

	# TODO: We might jump about a few hours here (date has not been
	#       told to give back values in UTC)
	date = date.replace(tzinfo=iso8601.UTC)

	if end_of_day and date.hour == 0 and date.minute == 0 and date.second == 0:
		date += datetime.timedelta(hours=23, minutes=59, seconds=59)

	return date.replace(tzinfo=iso8601.UTC)

def get_replies(made_by, pageno=1):
	#print('get_replies({}, {})'.format(made_by, pageno), file=sys.stderr)
	with requests_cache.disabled():
		page = requests.get('http://www.96boards.org/members/{}/forums/replies/page/{}/'.format(made_by, pageno))
	if not page.ok:
		return None
	tree = html.fromstring(page.content)

	authors = tree.xpath('//a[@class="bbp-author-name"]/text()')

	contents = tree.xpath('//div[@class="bbp-reply-content"]')
	contents = [ str(etree.tostring(c), 'utf-8') for c in contents ]

	dates = tree.xpath('//span[@class="bbp-reply-post-date"]/text()')
	dates = [ datetime.datetime.strptime(d,
			'%B %d, %Y at %I:%M %p').replace(tzinfo=iso8601.UTC)
			for d in dates ]

	links = tree.xpath('//a[@class="bbp-reply-permalink"]')
	links = [ l.get('href') for l in links ]

	page_data = [ get_page_data(uri) for uri in links ]
	if len(page_data):
		(titles, descriptions, topics, origps) = zip(*page_data)
	else:
		(titles, descriptions, topics, origps) = ([], [], [], [])

	posts = [ { 'author': a, 'original_poster': o, 'date': d, 'subject': t, 'description': s, 'topic': p, 'uri': l, 'content': c }
			for (a, o, d, t, s, p, l, c) in zip(authors, origps, dates, titles, descriptions, topics, links, contents)]

	return posts

def get_page_data(uri):
	#print('get_page_data({})'.format(uri), file=sys.stderr)

	page = requests.get(uri)
	tree = html.fromstring(page.content)
	titles = tree.xpath('//meta[@property="og:title"]')
	authors = tree.xpath('//a[@class="bbp-author-name"]/text()')

	# There may be more than one title. At present these differ only in
	# that some are post-fixed by the name of the blog. For that reason
	# we grab the common prefix (os.path.commonprefix does not require
	# that it be used on a path).
	title = os.path.commonprefix([ t.get('content') for t in titles ])

	descriptions = tree.xpath('//meta[@property="og:description"]')
	description = descriptions[0].get('content')

	path = [ p.strip() for p in description.split('Â»') ]
	try:
		path = path[path.index('Topics')+1:-1]
	except:
		path = ['Unclassified',]

	if len(authors) >= 1:
		original_poster = authors[0]
	else:
		original_poster = 'Anonymous'

	return (title, description, path, original_poster)

def get_count_by_topic(posts):
	count = collections.defaultdict(int)
	for p in posts:
		# The legend gets a little large if we include the path leading to
		# each sub-forum... lets just use the sub-forum name.
		#count['/'.join(p['topic'])] += 1
		count[p['topic'][-1]] += 1
	return count

def get_count_by_month(posts):
	count = collections.defaultdict(int)
	for p in posts:
		d = iso8601.parse_date(p['date'])
		count['{:04d}-{:02d}'.format(d.year, d.month)] += 1
	return count

def do_chart(args):
	posts = json.loads(sys.stdin.read())

	by_month_by_topic = collect_by(posts,
                lambda p: iso8601.parse_date(p['date']).strftime('%Y-%m'))

	for month in by_month_by_topic.keys():
		by_month_by_topic[month] = collect_and_count_by(
				by_month_by_topic[month],
				lambda p: p['topic'][-1])

	stacked_bar_chart(by_month_by_topic, args.output,
			title = 'Posts by month and topic',
			ylabel = 'Number of posts')

def do_count(args):
	print(len(json.loads(sys.stdin.read())))

def do_diffstat(args):
	diffs = []

	decoder = json.JSONDecoder()
	s = sys.stdin.read()
	while s:
		(d, i) = decoder.raw_decode(s)
		diffs += [d,]
		s = s[i:]

	diffstat = diffs[0]
	for newstat in diffs[1:]:
		diffstat['startdate'] = diffstat['date']
		diffstat['enddate'] = newstat['date']
		del diffstat['date']

		diffstat['topiccount'] = newstat['topiccount'] - diffstat['topiccount']
		diffstat['replycount'] = newstat['replycount'] - diffstat['replycount']

		by_title = {}
		for d in newstat['forums'] + newstat['subforums']:
			by_title[d['title']] = d

		for d in diffstat['forums'] + diffstat['subforums']:
			if d['title'] not in by_title:
				d['topiccount'] = -1
				d['replycount'] = -1
			else:
				d['topiccount'] = by_title[d['title']]['topiccount'] - d['topiccount']
				d['replycount'] = by_title[d['title']]['replycount'] - d['replycount']

		json.dump(diffstat, sys.stdout, indent=2)

		diffstat = newstat

def do_fetch(args):
	since = parse_date(args.since)

	if args.show_progress:
		sys.stderr.write('.')
		sys.stderr.flush()
	posts = get_replies(args.nick)

	# Fetch more pages until we reach the target date
	pageno = 2
	while posts[-1]['date'] > since:
		if args.show_progress:
			sys.stderr.write('.')
			sys.stderr.flush()

		more = get_replies(args.nick, pageno)
		if not more:
			break
		posts += more
		pageno += 1

	# Filter out any posts that are too old
	posts = [ p for p in posts if p['date'] > since ]

	# Sort by date (the output of the blog tool is almost, but not quite,
	# in reverse date order)
	posts = sorted(posts, key=lambda k: k['date'])

	for p in posts:
		p['date'] = p['date'].isoformat()
		p['id'] = p['uri'].split('-')[-1]
		p['nick'] = args.nick
		f = html.fragment_fromstring(p['content'])
		p['text'] = f.text_content().strip()
		p['wordcount'] = len(p['text'].split())

	json.dump(posts, sys.stdout)

def do_format(args):
	for p in json.loads(sys.stdin.read()):
		ln = args.template
		for m in re.finditer('{([^}:]+)([^}]*)}', args.template):
			field = m.group(1)
			fmt = m.group(2)

			try:
				if 'shortdate' == field:
					val = p['date'][:10]
				elif '-' in field:
					(field, attr) = field.split('-', 1)
					val = p[field][attr]
				else:
					val = p[field]
			except KeyError:
				continue

			ln = ln.replace(m.group(0), '{{{}}}'.format(fmt).format(val))
		print(ln)

def do_merge(args):
	decoder = json.JSONDecoder()

	s = sys.stdin.read()

	posts = []

	while s:
		(p, i) = decoder.raw_decode(s)
		posts += p
		s = s[i:]

	# Sort by date (iso8601 strings can be directly compared)
	posts = sorted(posts, key=lambda k: k['date'])

	json.dump(posts, sys.stdout)

def do_monthly(args):
	try:
		with open(os.environ['HOME']+'/.96btool-highlight_nicks') as f:
			hlnicks = json.load(f)
	except IOError:
		hlnicks = ()

	digest = {}
	for p in json.loads(sys.stdin.read()):
		if p['subject'] not in digest:
			digest[p['subject']] = p
			p['count'] = 1
		else:
			digest[p['subject']]['count'] += 1

	# Sort alphabetically by subject and re-index by topic
	by_topic = collections.defaultdict(list)
	for s in sorted(digest.keys()):
		topic = ' - '.join(digest[s]['topic'])
		by_topic[topic].append(digest[s])

	print('''\
<table border=2>
<tr>
<td>Board</td>
<td>Posts</td>
</tr>
''')

	for t in sorted(by_topic.keys()):
		total_posts = 0
		for s in by_topic[t]:
			total_posts += s['count']

		print('<tr>')
		print('<td>{}<br>{} post{}, {} topic{}</td>'.format(
			t, total_posts, 's' if total_posts > 1 else '',
			len(by_topic[t]), 's' if len(by_topic[t]) > 1 else ''))
		print('<td><ul>')
		for s in by_topic[t]:
			if s['original_poster'] in hlnicks:
				em = 'strong'
			else:
				em = 'em'
			print('<li>{} by <{}>{}</{}> (<a href="{}">{} post{}</a>)</li>'.format(
					s['subject'], em, s['original_poster'], em, s['uri'], s['count'], 's' if s['count'] > 1 else ''))
		print('</ul></td></tr>')
	print('</table>')

def add_percent_labels(labels, values):
	total = sum(values)
	return [ '{} ({:1.1f}%)'.format(l, 100 * (v / total)) for l, v in zip(labels, values) ]

def do_piechart(args):
	count = get_count_by_topic(json.loads(sys.stdin.read()))

	labels = [ k for k in sorted(count.keys()) ]
	counts = [ count[k] for k in labels ]
	colours = [ get_colour(k) for k in labels ]

	patches, texts = plt.pie(counts, colors=colours, startangle=90)
	plt.legend(patches, add_percent_labels(labels, counts), loc="best")
	plt.axis('equal')
	if args.title:
		plt.suptitle(args.title, fontsize=14)
	plt.tight_layout()
	plt.savefig(args.output)
	plt.close()

def do_filter(args):
	since = parse_date(args.since)
	until = parse_date(args.until)

	posts = json.loads(sys.stdin.read())

	posts = [ p for p in posts if iso8601.parse_date(p['date']) >= since ]
	posts = [ p for p in posts if iso8601.parse_date(p['date']) < until ]

	json.dump(posts, sys.stdout)

def do_stats(args):
	if args.loop:
		delta = datetime.timedelta(seconds=int(args.loop))
		alarm = datetime.datetime.now() + delta
		args.loop = None
		while True:
			do_stats(args)

			now = datetime.datetime.now()
			if now < alarm:
				time.sleep((alarm - now).total_seconds())
			alarm += delta

	with requests_cache.disabled():
		frontpage = requests.get('http://www.96boards.org/forums/')
		unreplied = requests.get('http://www.96boards.org/forums/view/no-replies/')
	if not frontpage.ok:
		return None

	now = datetime.datetime.now()
	stats = {'date': now.isoformat("T"),
		'subforums': [],
		'forums': []}

        #
        # Handle front page
        #

	tree = html.fromstring(frontpage.content)

	titles = tree.xpath('//a[@class="bbp-forum-title"]/text()')
	topiccounts = [ c.strip() for c in
			tree.xpath('//li[@class="bbp-forum-topic-count"]/text()')
			]
	topiccounts = [ int(c.replace(',', '')) for c in topiccounts[1::2] ]
	replycounts = [ c.strip() for c in
			tree.xpath('//li[@class="bbp-forum-reply-count"]/text()')
			]
	replycounts = [ int(c.replace(',', '')) for c in replycounts[1::2] ]

	stats['forums'] = [ { 'title': t, 'topiccount': c, 'replycount': r }
			for (t, c, r) in zip(titles, topiccounts, replycounts) ]

	stats['topiccount'] = sum(topiccounts)
	stats['replycount'] = sum(replycounts)

	subforums = tree.xpath('//a[@class="bbp-forum-link"]/text()')
	for s in subforums:
		(name, topics, posts) = re.match('(.*)[(]([0-9,]+) ([0-9,]+)[)]', s).groups()

		stats['subforums'].append({
			'title': name.strip(),
			'topiccount': int(topics.replace(',', '')),
			'replycount': int(posts.replace(',', ''))})

	#
        # Handle unreplied topics
	#

	tree = html.fromstring(unreplied.content)
	counts = tree.xpath('//div[@class="bbp-pagination-count"]/text()')
	stats['unreplied'] = int(
			re.search('\of ([0-9]+) total\)', counts[0]).group(1))


	#
	# Emit the data
	#

	if args.organise:
		os.makedirs(now.strftime('%Y/%m/%d'), exist_ok=True)
		with open(now.strftime('%Y/%m/%d/%H:%M:%S.json'), 'w') as f:
			json.dump(stats, f, indent=2)
		print('{}{:10}{:10}'.format(stats['date'][0:19],
				stats['topiccount'], stats['replycount']))
	else:
		json.dump(stats, sys.stdout, indent=2)

def do_weekly(args):
	wrappers = (
		textwrap.TextWrapper(),
		textwrap.TextWrapper(initial_indent=' * ', subsequent_indent='   '),
		textwrap.TextWrapper(initial_indent='   - ', subsequent_indent='     ')
	)

	def wrap(msg, level=1):
		print('\n'.join(wrappers[level].wrap(msg)))

	wrap("96Boards forum activity", level=1)

	summary = collections.defaultdict(int)
	for p in json.loads(sys.stdin.read()):
		summary[p['subject']] += 1

	for s in sorted(summary.keys()):
		if summary[s] == 1:
			wrap(s, level=2)
		else:
			wrap('{} ({} posts)'.format(s, summary[s]), level=2)

def main(argv):
	parser = argparse.ArgumentParser()
	subparsers = parser.add_subparsers(dest='sub-command')
	subparsers.required = True	# Can't be set using named arguments (yet)

	s = subparsers.add_parser('chart')
	s.add_argument("--output", default="96btool.png")
	s.set_defaults(func=do_chart)

	s = subparsers.add_parser('count')
	s.set_defaults(func=do_count)

	s = subparsers.add_parser('diffstat')
	s.set_defaults(func=do_diffstat)

	s = subparsers.add_parser('fetch')
	s.add_argument("--nick", default="danielt",
		        help="Nickname to fetch replies from");
	s.add_argument("--since", default="2012-01-01",
			help="When to gather information from")
	s.add_argument("--show-progress", action="store_true",
                        help="Show signs of progress whilst running")
	s.set_defaults(func=do_fetch)

	s = subparsers.add_parser('format')
	s.add_argument("--template",
			default="{id}: {subject} ({nick})")
	s.set_defaults(func=do_format)

	s = subparsers.add_parser('merge')
	s.set_defaults(func=do_merge)

	s = subparsers.add_parser('monthly')
	s.set_defaults(func=do_monthly)

	s = subparsers.add_parser('piechart')
	s.add_argument("--title", default=None)
	s.add_argument("--output", default="96btool.png")
	s.set_defaults(func=do_piechart)

	s = subparsers.add_parser('filter')
	s.add_argument("--since", default="2012-01-01",
			help="When to gather information from")
	s.add_argument("--until", default="tomorrow",
			help="When to stop gathering information")
	s.set_defaults(func=do_filter)

	s = subparsers.add_parser('stats')
	s.add_argument('--loop')
	s.add_argument('--organise', action='store_true')
	s.set_defaults(func=do_stats)

	s = subparsers.add_parser('summary')
	s.set_defaults(func=do_format,
		       template="{shortdate}: {subject} ({nick})")

	s = subparsers.add_parser('weekly')
	s.set_defaults(func=do_weekly)

	args = parser.parse_args(argv[1:])
	args.func(args)

if __name__ == "__main__":
	try:
		sys.exit(main(sys.argv))
	except KeyboardInterrupt:
		sys.exit(1)
	sys.exit(127)

