#!/usr/bin/env python3

'''
96btool - Tool to study a users forum posts

Prerequisites (ubuntu:16.04):

 * sudo apt -y install python3 python3-pip
 * sudo apt -y install python3-iso8601 python3-lxml python3-requests-cache

Reminders of interesting threads for a weekly report:

    96btool fetch --since 'next-friday -13 days' --nick danielt | \
    96btool filter --until 'next-friday -1 week' | \
    96btool weekly

Create list of links in case the reminders are not enough:

    96btool fetch --since 'next-friday -13 days' --nick danielt | \
    96btool filter --until 'next-friday -1 week' | \
    96btool format --template '{uri}' | cut -f1 -d'#' | sort -u


If you want to develop the tool itself to pull additional information from
the HTML you may need to experiment interactively with "real" HTML objects.
Try:

import requests
from lxml import html
page = requests.get('http://www.96boards.org/members/danielt/forums/replies')
tree = html.fromstring(page.content)
content = tree.xpath('//div[@class="bbp-reply-content"]')

'''

import argparse
import collections
import datetime
import iso8601
import hashlib
import json
import re
import requests
import requests_cache
import subprocess
import sys
import os
import textwrap
from lxml import html
from lxml import etree

# Let's make the plotting tools optional (for those who only use glimpse
# for their weekly report)
try:
	import matplotlib.pyplot as plt
except:
	pass

# Enable caching throughout
requests_cache.install_cache()

def get_colour(s):
	override = {
		'lava' : 'orange',
		'LAVA' : 'orange',
		'96boards' : 'red',
		'96Boards' : 'red',
	}

	if s in override:
		return override[s]

	return '#{}'.format(hashlib.md5(str(s).encode('UTF-8')).hexdigest()[-6:])

def parse_date(s, end_of_day=False):
	'''Convert a string to a datetime object.

	Operates by calling out to date (because it has a good relative date
	parser).

	'''
	date = subprocess.check_output(["date", "-d", s, "+%Y-%m-%d %H:%M:%S"])
	date = str(date, 'utf-8')
	date = date.rstrip()
	date = datetime.datetime.strptime(date, "%Y-%m-%d %H:%M:%S")

	# TODO: We might jump about a few hours here (date has not been
	#       told to give back values in UTC)
	date = date.replace(tzinfo=iso8601.UTC)

	if end_of_day and date.hour == 0 and date.minute == 0 and date.second == 0:
		date += datetime.timedelta(hours=23, minutes=59, seconds=59)

	return date.replace(tzinfo=iso8601.UTC)

def get_replies(made_by, pageno=1):
	#print('get_replies({}, {})'.format(made_by, pageno), file=sys.stderr)
	with requests_cache.disabled():
		page = requests.get('http://www.96boards.org/members/{}/forums/replies/page/{}/'.format(made_by, pageno))
	if not page.ok:
		return None
	tree = html.fromstring(page.content)

	authors = tree.xpath('//a[@class="bbp-author-name"]/text()')

	contents = tree.xpath('//div[@class="bbp-reply-content"]')
	contents = [ str(etree.tostring(c), 'utf-8') for c in contents ]

	dates = tree.xpath('//span[@class="bbp-reply-post-date"]/text()')
	dates = [ datetime.datetime.strptime(d,
			'%B %d, %Y at %I:%M %p').replace(tzinfo=iso8601.UTC)
			for d in dates ]

	links = tree.xpath('//a[@class="bbp-reply-permalink"]')
	links = [ l.get('href') for l in links ]

	page_data = [ get_page_data(uri) for uri in links ]
	if len(page_data):
		(titles, descriptions, topics) = zip(*page_data)
	else:
		(titles, descriptions, topics) = ([], [], [])

	posts = [ { 'author': a, 'date': d, 'subject': t, 'description': s, 'topic': p, 'uri': l, 'content': c }
			for (a, d, t, s, p, l, c) in zip(authors, dates, titles, descriptions, topics, links, contents)]

	return posts

def get_page_data(uri):
	#print('get_page_data({})'.format(uri), file=sys.stderr)

	page = requests.get(uri)
	tree = html.fromstring(page.content)
	titles = tree.xpath('//meta[@property="og:title"]')

	# There may be more than one title. At present these differ only in
	# that some are post-fixed by the name of the blog. For that reason
	# we grab the common prefix (os.path.commonprefix does not require
	# that it be used on a path).
	title = os.path.commonprefix([ t.get('content') for t in titles ])

	descriptions = tree.xpath('//meta[@property="og:description"]')
	description = descriptions[0].get('content')

	path = [ p.strip() for p in description.split('Â»') ]
	try:
		path = path[path.index('Topics')+1:-1]
	except:
		path = ['Unclassified',]

	return (title, description, path)

def get_count_by_topic(posts):
	count = collections.defaultdict(int)
	for p in posts:
		# The legend gets a little large if we include the path leading to
		# each sub-forum... lets just use the sub-forum name.
		#count['/'.join(p['topic'])] += 1
		count[p['topic'][-1]] += 1
	return count

def get_count_by_month(posts):
	count = collections.defaultdict(int)
	for p in posts:
		d = iso8601.parse_date(p['date'])
		count['{:04d}-{:02d}'.format(d.year, d.month)] += 1
	return count

def do_chart(args):
	posts = json.loads(sys.stdin.read())

	by_month = collections.defaultdict(list)
	for p in posts:
		d = iso8601.parse_date(p['date'])
		by_month['{:04d}-{:02d}'.format(d.year, d.month)].append(p)

	by_month_by_topic = dict(by_month)
	for month in by_month_by_topic.keys():
		count = collections.defaultdict(int)
		for p in by_month_by_topic[month]:
			#topic = '/'.join(p['topic'])
			topic = p['topic'][-1]
			count[topic] += 1
		by_month_by_topic[month] = count

	topics = set()
	for month in by_month_by_topic.values():
		topics |= set(month.keys())
	topics = sorted(topics)

	width = 1
	left = [ x+(1-width)/2 for x in range(len(by_month_by_topic)) ]
	labels = sorted(by_month_by_topic.keys())

	plots = []
	for m in topics:
		values = [ by_month_by_topic[l][m] for l in labels ]
		plots.append(plt.bar(left, values, width, color=get_colour(m)))

	plt.ylabel('Number of posts')
	#plt.xlabel('Posts by month and topic')
	plt.title('Posts by month and topic')
	plt.xticks([ x+0.5 for x in range(len(by_month_by_topic)) ], labels, rotation=90)
	plt.legend(plots, topics, loc='best')
	plt.tight_layout()
	plt.savefig(args.output)
	plt.close()

def do_count(args):
	print(len(json.loads(sys.stdin.read())))

def do_fetch(args):
	since = parse_date(args.since)

	if args.show_progress:
		sys.stderr.write('.')
		sys.stderr.flush()
	posts = get_replies(args.nick)

	# Fetch more pages until we reach the target date
	pageno = 2
	while posts[-1]['date'] > since:
		if args.show_progress:
			sys.stderr.write('.')
			sys.stderr.flush()

		more = get_replies(args.nick, pageno)
		if not more:
			break
		posts += more
		pageno += 1

	# Filter out any posts that are too old
	posts = [ p for p in posts if p['date'] > since ]

	# Sort by date (the output of the blog tool is almost, but not quite,
	# in reverse date order)
	posts = sorted(posts, key=lambda k: k['date'])

	for p in posts:
		p['date'] = p['date'].isoformat()
		p['id'] = p['uri'].split('-')[-1]
		p['nick'] = args.nick
		f = html.fragment_fromstring(p['content'])
		p['text'] = f.text_content().strip()
		p['wordcount'] = len(p['text'].split())

	json.dump(posts, sys.stdout)

def do_format(args):
	for p in json.loads(sys.stdin.read()):
		ln = args.template
		for m in re.finditer('{([^}:]+)([^}]*)}', args.template):
			field = m.group(1)
			fmt = m.group(2)

			try:
				if 'shortdate' == field:
					val = p['date'][:10]
				elif '-' in field:
					(field, attr) = field.split('-', 1)
					val = p[field][attr]
				else:
					val = p[field]
			except KeyError:
				continue

			ln = ln.replace(m.group(0), '{{{}}}'.format(fmt).format(val))
		print(ln)

def do_merge(args):
	decoder = json.JSONDecoder()

	s = sys.stdin.read()

	posts = []

	while s:
		(p, i) = decoder.raw_decode(s)
		posts += p
		s = s[i:]

	# Sort by date (iso8601 strings can be directly compared)
	posts = sorted(posts, key=lambda k: k['date'])

	json.dump(posts, sys.stdout)

def do_monthly(args):
	digest = {}
	for p in json.loads(sys.stdin.read()):
		if p['subject'] not in digest:
			digest[p['subject']] = p
			p['count'] = 1
		else:
			digest[p['subject']]['count'] += 1

	# Sort alphabetically by subject and re-index by topic
	by_topic = collections.defaultdict(list)
	for s in sorted(digest.keys()):
		topic = ' - '.join(digest[s]['topic'])
		by_topic[topic].append(digest[s])

	print('<ul>')
	for t in sorted(by_topic.keys()):
		print('<li>{}'.format(t))
		for s in by_topic[t]:
			print('<ul>')
			print('<li>{} (<a href="{}">{} post{}</a>)</li>'.format(
					s['subject'], s['uri'], s['count'], 's' if s['count'] > 1 else ''))
			print('</ul>')
			
		print('</li>')
	print('</ul>')

def do_piechart(args):
	count = get_count_by_topic(json.loads(sys.stdin.read()))

	labels = [ k for k in sorted(count.keys()) ]
	counts = [ count[k] for k in labels ]
	colours = [ get_colour(k) for k in labels ]

	patches, texts = plt.pie(counts, colors=colours, startangle=90)
	plt.legend(patches, labels, loc="best")
	plt.axis('equal')
	if args.title:
		plt.suptitle(args.title, fontsize=14)
	plt.tight_layout()
	plt.savefig(args.output)
	plt.close()

def do_filter(args):
	since = parse_date(args.since)
	until = parse_date(args.until)

	posts = json.loads(sys.stdin.read())

	posts = [ p for p in posts if iso8601.parse_date(p['date']) >= since ]
	posts = [ p for p in posts if iso8601.parse_date(p['date']) < until ]

	json.dump(posts, sys.stdout)

def do_weekly(args):
	wrappers = (
		textwrap.TextWrapper(),
		textwrap.TextWrapper(initial_indent=' * ', subsequent_indent='   '),
		textwrap.TextWrapper(initial_indent='   - ', subsequent_indent='     ')
	)

	def wrap(msg, level=1):
		print('\n'.join(wrappers[level].wrap(msg)))

	wrap("96Boards forum activity", level=1)

	summary = collections.defaultdict(int)
	for p in json.loads(sys.stdin.read()):
		summary[p['subject']] += 1

	for s in sorted(summary.keys()):
		if summary[s] == 1:
			wrap(s, level=2)
		else:
			wrap('{} ({} posts)'.format(s, summary[s]), level=2)

def main(argv):
	parser = argparse.ArgumentParser()
	subparsers = parser.add_subparsers(dest='sub-command')
	subparsers.required = True	# Can't be set using named arguments (yet)

	s = subparsers.add_parser('chart')
	s.add_argument("--output", default="96btool.png")
	s.set_defaults(func=do_chart)

	s = subparsers.add_parser('count')
	s.set_defaults(func=do_count)

	s = subparsers.add_parser('fetch')
	s.add_argument("--nick", default="danielt",
		        help="Nickname to fetch replies from");
	s.add_argument("--since", default="2012-01-01",
			help="When to gather information from")
	s.add_argument("--show-progress", action="store_true",
                        help="Show signs of progress whilst running")
	s.set_defaults(func=do_fetch)

	s = subparsers.add_parser('format')
	s.add_argument("--template",
			default="{id}: {subject} ({nick})")
	s.set_defaults(func=do_format)

	s = subparsers.add_parser('merge')
	s.set_defaults(func=do_merge)

	s = subparsers.add_parser('monthly')
	s.set_defaults(func=do_monthly)

	s = subparsers.add_parser('piechart')
	s.add_argument("--title", default=None)
	s.add_argument("--output", default="96btool.png")
	s.set_defaults(func=do_piechart)

	s = subparsers.add_parser('filter')
	s.add_argument("--since", default="2012-01-01",
			help="When to gather information from")
	s.add_argument("--until", default="tomorrow",
			help="When to stop gathering information")
	s.set_defaults(func=do_filter)

	s = subparsers.add_parser('summary')
	s.set_defaults(func=do_format,
		       template="{shortdate}: {subject} ({nick})")

	s = subparsers.add_parser('weekly')
	s.set_defaults(func=do_weekly)

	args = parser.parse_args(argv[1:])
	args.func(args)

if __name__ == "__main__":
	try:
		sys.exit(main(sys.argv))
	except KeyboardInterrupt:
		sys.exit(1)
	sys.exit(127)

