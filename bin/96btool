#!/usr/bin/env python3

'''
96btool - Tool to study a users forum posts

Prerequisites (ubuntu:16.04):

 * sudo apt -y install python3 python3-pip
 * sudo apt -y install python3-iso8601

Reminders of interesting threads for a weekly report:

    96btool pull --pipe | \
    96btool filter \
	--user danielt \
	--since 'next-friday -13 days' \
	--until 'next-friday -1 week' | \
    96btool weekly
'''

import argparse
import collections
import datetime
import iso8601
import json
import re
import subprocess
import sys
import pydiscourse
import os
import textwrap
import time
import traceback

import toys.chart as chart
import toys.collect as collect
import toys.config as config

# If it's installed we'd rather use IPython for interaction...
try:
	import IPython
	interact = IPython.embed
except:
	import pdb
	interact = pdb.set_trace

# To keep this up-to-date try:
#   filter --grep "About the.*category"
category_lookup = {
	1: 'Uncategorized',
	4: 'Staff',
	5: 'General',
	6: 'Specification',
	7: 'Products',
	9: 'HiKey',
	10: 'DragonBoard410c',
	11: 'Bubblegum-96',
	13: 'MediaTek-X20',
	15: 'Carbon',
	16: 'Poplar Board',
	17: 'Mezzanine Support',
	18: 'OpenHours',
	19: 'DragonBoard820c'
}

def parse_date(s, end_of_day=False):
	'''Convert a string to a datetime object.

	Operates by calling out to date (because it has a good relative date
	parser).

	'''
	date = subprocess.check_output(["date", "-d", s, "+%Y-%m-%d %H:%M:%S"])
	date = str(date, 'utf-8')
	date = date.rstrip()
	date = datetime.datetime.strptime(date, "%Y-%m-%d %H:%M:%S")

	# TODO: We might jump about a few hours here (date has not been
	#       told to give back values in UTC)
	date = date.replace(tzinfo=iso8601.UTC)

	if end_of_day and date.hour == 0 and date.minute == 0 and date.second == 0:
		date += datetime.timedelta(hours=23, minutes=59, seconds=59)

	return date.replace(tzinfo=iso8601.UTC)

class Post(dict):
	post_db = {}
	topic_db = {}
	username_db = {}
	user_db = {}

	def created_at(self):
		return iso8601.parse_date(self['created_at'])

	def get_user(self, client):
		if self['username'] not in Post.user_db:
			time.sleep(0.1)
			Post.user_db[self['username']] = client.user(self['username'])
		self['user'] = Post.user_db[self['username']]
		return self['user']

	def lint(self):
		'''Fix minor errors and inefficiencies with a post.

		Returns true of the entire post should be discarded.
		'''
		if 'user' in self:
			Post.lint_user(self['user'])

		if 'topic' in self:
			Post.lint_topic(self['topic'])

			# Many Linaro staff are admins with access to PMs but we
			# never want to make local copies of these. This is
			# mostly respect for privacy although, in truth, they
			# would also confuse the statistics!
			return self['topic']['archetype'] == 'private_message'

		return False

	@staticmethod
	def fetch(client, from_id, to_id, verbose=False):
		for post_id in range(from_id, to_id):
			try:
				time.sleep(0.5)
				post = client._get('/posts/{}.json'.format(post_id))
				post = Post(post)
				if post.lint():
					if verbose:
						sys.stdout.write('P')
						sys.stdout.flush()
					continue

			except pydiscourse.exceptions.DiscourseClientError:
				if verbose:
					sys.stdout.write('E')
					sys.stdout.flush()
				continue

			result = '.'

			if post['topic_id'] not in Post.topic_db:
				time.sleep(0.1)
				topic = client._get('/t/{}.json'.format(
						post['topic_id']))
				result = 'o'
			else:
				topic = Post.topic_db[post['topic_id']]
			post['topic'] = topic

			post.get_user(client)

			post.lint()
			post.index(post)

			if verbose:
				sys.stdout.write(result)
				sys.stdout.flush()

	@staticmethod
	def index(posts):
		if isinstance(posts, Post):
			posts = (posts,)
		for p in posts:
			Post.post_db[p['id']] = p
			if 'topic' in p:
				Post.topic_db[p['topic_id']] = p['topic']
			if 'username' in p:
				Post.username_db[p['user_id']] = p['username']
			if 'user' in p:
				Post.user_db[p['user_id']] = p['user']

	@staticmethod
	def latest(client):
		# Grab the latest topics (we can use these to figure out the
		# most recent post_id)
		latest_topics = client.latest_topics()

		# the 0th topic is the freshest and should have the largest
		# post id
		latest_topic_id = latest_topics['topic_list']['topics'][0]['id']
		latest_topic = client._get('/t/{}.json'.format(latest_topic_id))
		latest_post_id = max(latest_topic['post_stream']['stream'])

		# Having downloaded it we may as well cache it
		Post.lint_topic(latest_topic)
		Post.topic_db['latest_topic_id'] = latest_topic

		return latest_post_id

	@staticmethod
	def lint_topic(t):
		if 'post_stream' in t:
			del t['post_stream']
		if 'details' in t:
			del t['details']
		if 'category' not in t:
			t['category'] = 'UNKNOWN'
		if t['category'] == 'UNKNOWN':
			if not t['category_id']:
				t['category'] = 'None'
			elif t['category_id'] in category_lookup:
				t['category'] = category_lookup[t['category_id']]
			else:
				print('WARNING: Unknown category id: {}'.format(
						t['category_id']),
						file=sys.stderr)
				t['category'] = 'UNKNOWN'
		if 'username' not in t or t['username'] == 'unknown':
			if t['user_id'] in Post.username_db:
				t['username'] = Post.username_db[t['user_id']]
			else:
				t['username'] = 'unknown'
		if 'uri' not in t:
			t['uri'] = 'https://discuss.96boards.org/t/{}'.format(
					t['slug'])

		return t

	@staticmethod
	def lint_user(u):
		'''Drop all info about a user except for a narrow whitelist'''
		keep = ( 'email', 'id', 'name', 'username', 'website' )
		for k in list(u):
			if k not in keep:
				del u[k]
		return u

	@staticmethod
	def load(obj):
		if not obj:
			data = json.load(sys.stdin)
		elif isinstance(obj, str):
			with open(obj, 'r') as f:
				data = json.load(f)
		else:
			data = json.load(obj)

		return [ Post(p) for p in data ]

def do_chart(args):
	'''Visualise the post data as a stacked bar chart'''
	posts = Post.load(args.json)

	by_month_by_category = collect.accumulate_2d(posts,
                lambda p: iso8601.parse_date(p['created_at']).strftime('%Y-%m'),
		lambda p: p['topic']['category'])

	chart.stacked_barchart(by_month_by_category, args.output,
			title = 'Posts by month and category',
			ylabel = 'Number of posts')

def do_count(args):
	'''Count posts, potential organising them into categories first'''
	posts = Post.load(args.json)
	count = None

	if args.by_category:
		count = collect.accumulate(posts,
				lambda x: x['topic']['category'])

	if args.by_user:
		count = collect.accumulate(posts, lambda x: x['username'])

	if count:
		flat_count = [ (k, v) for k, v in count.items() ]
		if args.rank:
			flat_count = sorted(flat_count, key=lambda x: x[1], reverse=True)
		else:
			flat_count = sorted(flat_count, key=lambda x: x[0])

		if args.csv:
			for k, v in flat_count:
				print('"{}",{}'.format(k, v))
		elif args.text:
			for k, v in flat_count:
				print('{:30} {}'.format(k, v))
		else:
			if args.rank:
				count = flat_count
			json.dump(count, sys.stdout, sort_keys=True, indent=2)
	else:
		print(len(posts))

def do_dump(args):
	'''Dump the local database to standard output'''
	with open(args.db) as f:
		sys.stdout.write(f.read())

def do_fetch(args):
	'''Use discourse to search for matching topics (limited to <50)'''
	since = parse_date(args.since)
	after = since.strftime(' after:%Y-%m-%d')

	cfg = config.get_config()
	client = pydiscourse.DiscourseClient( cfg['96btool']['server'],
			api_username=cfg['96btool']['username'],
			api_key=config.get_password(cfg, '96btool'),
			timeout=5)

	results = client._get('/search.json', **{ 'q': args.query + after })

	posts = results['posts']
	if len(posts) == 50:
		print("Too many results, limited to 50", file=sys.stderr)

	topics = {}
	for post in posts:
		topic_id = post['topic_id']
		if topic_id not in topics:
			topic = client._get('/t/{}.json'.format(topic_id))
			del topic['post_stream']
			del topic['details']
			topics[topic_id] = topic
		post['topic'] = topics[topic_id]

	# Sort by date
	posts = sorted(posts, key=lambda k: k['created_at'])

	json.dump(posts, sys.stdout, indent=2)

def do_filter(args):
	'''Filter posts using various criteria (including date of post)'''
	since = parse_date(args.since)
	until = parse_date(args.until)

	posts = Post.load(args.json)

	posts = [ p for p in posts if iso8601.parse_date(p['created_at']) >= since ]
	posts = [ p for p in posts if iso8601.parse_date(p['created_at']) < until ]

	if args.user:
		posts = [ p for p in posts if p['username'] == args.user ]

	if args.grep:
		e = re.compile(args.grep)
		posts = [ p for p in posts if
				re.search(e, p['topic']['title']) or
				re.search(e, p['raw']) ]

	json.dump(posts, sys.stdout, indent=2)

def do_format(args):
	'''Summarize the data in a custom text format'''
	posts = Post.load(args.json)
	for p in posts:
		ln = args.template
		for m in re.finditer('{([^}:]+)([^}]*)}', args.template):
			field = m.group(1)
			fmt = m.group(2)

			try:
				if 'shortdate' == field:
					val = p['created_at'][:10]
				elif '-' in field:
					(field, attr) = field.split('-', 1)
					val = p[field][attr]
				else:
					val = p[field]
			except KeyError:
				continue

			ln = ln.replace(m.group(0), '{{{}}}'.format(fmt).format(val))
		print(ln)

def do_interact(args):
	'''Directly interact with the JSON data'''
	posts = Post.load(args.json)
	post = posts[-1]

	cfg = config.get_config()
	client = pydiscourse.DiscourseClient( cfg['96btool']['server'],
			api_username=cfg['96btool']['username'],
			api_key=config.get_password(cfg, '96btool'),
			timeout=5)

	interact()

def do_merge(args):
	'''Read multiple concatenated JSON files and merge into one'''
	decoder = json.JSONDecoder()

	s = sys.stdin.read()

	posts = []

	while s:
		(p, i) = decoder.raw_decode(s)
		posts += p
		s = s[i:]

	# De-duplicate and sort by id
	posts = [ Post(p) for p in posts ]
	Post.index(posts)
	posts = [ Post.post_db[k] for k in sorted(Post.post_db.keys()) ]

	json.dump(posts, sys.stdout, sort_keys=True, indent=2)

def do_monthly(args):
	'''Generate a monthly activity report'''
	try:
		with open(os.environ['HOME']+'/.96btool-highlight_nicks') as f:
			hlnicks = json.load(f)
	except IOError:
		hlnicks = ()

	digest = {}
	for p in Post.load(args.json):
		if p['topic']['title'] not in digest:
			digest[p['topic']['title']] = p
			p['count'] = 1
		else:
			digest[p['topic']['title']]['count'] += 1

	# Sort alphabetically by subject and re-index by topic
	by_topic = collections.defaultdict(list)
	for s in sorted(digest.keys()):
		topic = digest[s]['topic']['category']
		by_topic[topic].append(digest[s])

	print('''\
<table border=2>
<tr>
<td>Board</td>
<td>Posts</td>
</tr>
''')

	for t in sorted(by_topic.keys()):
		total_posts = 0
		for s in by_topic[t]:
			total_posts += s['count']

		print('<tr>')
		print('<td>{}<br>{} post{}, {} topic{}</td>'.format(
			t, total_posts, 's' if total_posts > 1 else '',
			len(by_topic[t]), 's' if len(by_topic[t]) > 1 else ''))
		print('<td><ul>')
		for s in by_topic[t]:
			if s['topic']['username'] in hlnicks:
				em = 'strong'
			else:
				em = 'em'
			print('<li>{} by <{}>{}</{}> (<a href="{}">{} post{}</a>)</li>'.format(
					s['topic']['title'], em, s['topic']['username'], em, s['topic']['uri'], s['count'], 's' if s['count'] > 1 else ''))
		print('</ul></td></tr>')
	print('</table>')

def do_passwd(args):
	'''Store a password in the keyring'''
	print("WARNING: This is *not* your forum password. It is an API key")
	print("         issued to allow JSON access to discourse.")
	cfg = config.get_config()
	config.set_password(cfg, '96btool')

def do_piechart(args):
	'''Visualise the post data as a pie chart'''
	posts = Post.load(args.json)
	count = collect.accumulate(posts, lambda p: p['topic']['category'])
	chart.piechart(count, args.output, args.title)

def do_pull(args):
	'''Update the local cache of the database'''
	cfg = config.get_config()
	client = pydiscourse.DiscourseClient( cfg['96btool']['server'],
			api_username=cfg['96btool']['username'],
			api_key=config.get_password(cfg, '96btool'),
			timeout=5)

	if args.verbose:
		sys.stdout.write('Reading existing post cache .')
		sys.stdout.flush()
	try:
		posts = Post.load(args.db)
		Post.index(posts)

		if args.verbose:
			sys.stdout.write('..')
			sys.stdout.flush()
	except (FileNotFoundError, ValueError):
		if args.verbose:
			sys.stdout.write('.. file not found -')

	if len(Post.post_db) and not args.refresh:
		max_post_id = max([ i for i in Post.post_db.keys() ])
	else:
		max_post_id = 0

	# Grab the latest topics (we can use these to figure out the
	# most recent post_id)
	if args.verbose:
		print(' {} posts'.format(len(Post.post_db)))
		sys.stdout.write('Looking at the latest topics ...')
		sys.stdout.flush()
	latest_post_id = Post.latest(client)

	# Start grabbing the posts
	if args.verbose:
		sys.stdout.write(' ok\nFetching posts from #{} to #{} .'.format(
				max_post_id + 1, latest_post_id))
		sys.stdout.flush()
	try:
		# fetch automatically indexes as it works so we can ignore
		# the return value here...
		Post.fetch(client, max_post_id+1, latest_post_id+1,
				verbose=args.verbose)
		retval = 0

	except BaseException as e:
		if args.verbose:
			sys.stdout.write(' abort (and save progress)\n')
		if args.debug:
			traceback.print_exc()
		else:
			print('ERROR: {} ...'.format(str(e)), file=sys.stderr)
		retval = 1
	finally:
		# Final tidy up (reconstruct posts from the post_db)
		if args.verbose:
			sys.stdout.write(
				' ok\nPerforming final cleanup ...'.format(
					retval))
			sys.stdout.flush()
		posts = [ Post.post_db[k] for k in sorted(Post.post_db.keys()) ]

		with open(args.db, 'w') as f:
			json.dump(posts, f, sort_keys=True, indent=2)

		if args.verbose:
			print(' ok')

	if args.pipe:
		json.dump(posts, sys.stdout, indent=2)

	return retval

def do_refresh(args):
	'''Refresh the local database and fix minor integrity errors'''
	cfg = config.get_config()
	client = pydiscourse.DiscourseClient( cfg['96btool']['server'],
			api_username=cfg['96btool']['username'],
			api_key=config.get_password(cfg, '96btool'),
			timeout=5)

	posts = Post.load(args.db)
	Post.index(posts)

	if args.users:
		with open(args.users, 'r') as f:
			users = json.load(f)
		for u in users:
			Post.user_db[u['username']] = u

	# TODO: try to download any missing post ids

	try:
		for p in posts:
			if args.users:
				if p['username'] not in Post.user_db:
					print('No info for user {}'.format(p['username']), file=sys.stderr)
				else:
					p['user'] = Post.user_db[p['username']]
			else:
				if 'user' not in p and p['username']:
					if args.verbose and p['username'] not in Post.user_db:
						print('Refreshing user {}'.format(p['username']), file=sys.stderr)
					p.get_user(client)
			if p.lint():
				del Post.post_db[p['id']]
	except BaseException as e:
		traceback.print_exc()
	finally:
		posts = [ Post.post_db[k] for k in sorted(Post.post_db.keys()) ]

		with open(args.db, 'w') as f:
			json.dump(posts, f, sort_keys=True, indent=2)

def do_weekly(args):
	'''Generate a detailed weekly activity report'''
	wrappers = (
		textwrap.TextWrapper(),
		textwrap.TextWrapper(initial_indent=' * ', subsequent_indent='   '),
		textwrap.TextWrapper(initial_indent='   - ', subsequent_indent='     ')
	)

	def wrap(msg, level=1):
		print('\n'.join(wrappers[level].wrap(msg)))

	wrap("96Boards forum activity", level=1)

	summary = collections.defaultdict(int)
	for p in Post.load(args.json):
		summary[p['topic']['title']] += 1

	for s in sorted(summary.keys()):
		if summary[s] == 1:
			wrap(s, level=2)
		else:
			wrap('{} ({} posts)'.format(s, summary[s]), level=2)

def main(argv):
	defaultdb=os.path.dirname(os.path.realpath(sys.argv[0])) + '/../96btool.db'

	parser = argparse.ArgumentParser()
	subparsers = parser.add_subparsers(dest='sub-command')
	subparsers.required = True	# Can't be set using named arguments (yet)

	def new_parser(f, no_json_arg=False):
		assert(f.__name__.startswith('do_'))
		name = f.__name__[3:]
		helptext = f.__doc__
		s = subparsers.add_parser(name, help=helptext)
		s.set_defaults(func=f)
		if not no_json_arg:
			s.add_argument('json', nargs='?',
					help='Read data from this file')

		return s

	s = new_parser(do_chart)
	s.add_argument("--output", default="96btool.png")

	s = new_parser(do_count)
	s.add_argument('--csv', action='store_true',
		       help="Show the count data as CSV")
	s.add_argument('--by-category', action='store_true',
			help="Count posts in each category")
	s.add_argument('--by-user', action='store_true',
			help="Count posts by each user")
	s.add_argument('--rank', action='store_true',
			help="Sort the data into reverse numeric order")
	s.add_argument('--text', action='store_true',
			help="Show results in plain text")

	s = new_parser(do_dump, no_json_arg=True)
	s.add_argument('--db', default=defaultdb,
		       help="File to update")

	s = new_parser(do_fetch, no_json_arg=True)
	s.add_argument("--query", default="@danielt",
		        help="Nickname to fetch replies from");
	s.add_argument("--since", default="2012-01-01",
			help="When to gather information from")

	s = new_parser(do_filter)
	s.add_argument("--grep",
			help="Search for a string within a post")
	s.add_argument("--since", default="2012-01-01",
			help="When to gather information from")
	s.add_argument("--until", default="tomorrow",
			help="When to stop gathering .information")
	s.add_argument("--user", help="Select only posts by this user")

	s = new_parser(do_format)
	s.add_argument("--template",
			default="{id}: {topic-title} ({username})")

	s = new_parser(do_interact)
	s.add_argument("json", nargs='?', default=defaultdb)

	s = new_parser(do_merge, no_json_arg=True)

	s = new_parser(do_monthly)

	s = new_parser(do_passwd, no_json_arg=True)

	s = new_parser(do_piechart)
	s.add_argument("--title", default=None)
	s.add_argument("--output", default="96btool.png")

	s = new_parser(do_pull, no_json_arg=True)
	s.add_argument('--debug', action='store_true',
		       help="Extra diagnostics")
	s.add_argument('--db', default=defaultdb,
		       help="File to update")
	s.add_argument('--pipe', action='store_true',
		       help="Duplicate output on stdout")
	s.add_argument('--refresh', action='store_true',
		       help="Try to fetch posts missing due to previous errors")
	s.add_argument('--verbose', action='store_true',
		       help="Show internal workings")

	s = new_parser(do_refresh, no_json_arg=True)
	s.add_argument('--db', default=defaultdb,
		       help="File to update")
	s.add_argument('--verbose', action='store_true',
		       help="Show internal workings")
	s.add_argument('--users', help='Use a local copy of the user list')

	s = subparsers.add_parser('summary', help=do_format.__doc__)
	s.set_defaults(func=do_format,
		       template="{shortdate}: {topic-title} ({username})")
	s.add_argument('json', nargs='?',
			help='Read data from this file')

	s = new_parser(do_weekly)

	args = parser.parse_args(argv[1:])
	return args.func(args)

if __name__ == "__main__":
	try:
		sys.exit(main(sys.argv))
	except KeyboardInterrupt:
		sys.exit(1)
	sys.exit(127)

